* In this document we are going to re-organize repo from add_nix_serve_flake.org and add ops components
- A few functions we might try to run from opsshell include:
- Containerization, try to containerize our entrypoint
- Standing up a new aws instance
- build/run binaryCache flake on new instance.
- As a side quest I want to add a second attic server flake to this repo.https://docs.attic.rs/tutorial.html
** Making copy of nix-binary-cache re-organinzing a bit
- I need to investigate structure from cardano-world and yumi-env to see how I should organize my cells, cell blocks
- In std-nix video, cellblock would be app-name, cells would be package, operable and OCI
- I am going to try:
#+begin_example 
[root] flake.nix
  [nix] shells app cache-server
    [shells] one cell per env or single cell.nix with multiple environments in it?
    [rust-app] app.nix operable.nix oci.nix
    [cache-server] chache-server.nix operable.nix oci.nix
#+end_example
- Looking at cardano-world, they have devshells embedded with automation that references profiles in other cellBlocks.
- In yumi-env we have shells embedded with dev, I think I will follow that pattern adding ops with my shell in there and then same level app, cache server
- My new structure:
#+begin_example
[root] flake.nix
  [nix] ops rust-app ops cache-server
    [ops] opsshell.nix
    [rust-app] apps.nix toolchain.nix
    [cache-server] entrypoints.nix containers.nix
#+end_example
** Add OCI container
- Current:
- Re-organize the repo to match yumi-env a bit better
- Starting nix shell I can std run build and enter app, nix-cache, devshells
- I can nix build the flake, note I still do not know how to combine packages, still need more info on std.harvest inputs
- All parts of nix/std/shell interaction working as expected
- To add OCI start by adding the following containers.nix file to our cache-server directory:
- Note it consumes the operable we will define in entrypoints.
#+begin_example
{
  inputs,
  cell,
}: let
   inherit (inputs) nixpkgs std;
   l = nixpkgs.lib // builtins;

   name = "nix-cache-server"
   operable = cell.entrypoints.default;
in {
   nix-cache-server = std.lib.ops.mkStandardOCI {
     inherit name operable;
   };

}
#+end_example
- To make this work I need to turn my entrypoint into an operable with:
#+begin_example
# Because mkOperable expects a package I set it here so I can call it in the inherit, 
# When I tried "inherit nix-serve" it complained expecting package.
# In repo the nix-cache-server is defined under default in entrypoint.nix, 
# TODO Seperate nix-serve.default to seperate packages.nix we can refer to her
 package = nix-serve;
in
{
 nix-cache-server = lib.ops.mkOperable {
      inherit package;
      runtimeInputs = [nix-serve];
      runtimeScript = ''
       ${nix-serve}/bin/nix-serve --port 8080
      '';
  };
}
#+end_example
- I was able to build the flake and start the nix-cache-server using std
- I was able to build the contianer with:
#+begin_example
std //cache-server/containers/nix-cache-server:load 
#+end_example
- I was able to run the container with:
#+begin_example
docker run nix-cache-server:yyvmmf9qzmjpl11sg0aly3svzyrjnjr0 -p 8080:8080
#+end_example
- But I do not see port 8080 bound on my host and adding debug entrypoint lets me run the binary, but does not explain why I do not see the port locally.
- There are some concern that the issue might be related to local docker state, so intent is to spin up nix-node in aws and try on there
- Spin up new aws instance to test on
#+begin_src tmux :session s1
export NIXPKGS_ALLOW_UNFREE=1
#+end_src
#+begin_src tmux :session s1
nix-env -i ec2-api-tool
#+end_src
#+begin_src tmux :session s1
ec2-run-instances -t t2.large --region ap-southeast-2 -W [aws_secret_access_key]  -O [aws_access_key_id] -b '/dev/xvda=:30' -k gsg-keypair ami-0638db75ba113c635
#+end_src
- Use previously generated keys to ssh to new instance:
#+begin_src tmux :session s1
ssh -i /tmp/gsg-keypair.pem root@3.25.252.5
#+end_src
- Allow experimental feutures
#+begin_src tmux :session s1
echo "experimental-features = nix-command flakes" > .config/nix/nix.conf
#+end_src
- Add std to our shell
#+begin_src tmux :session s1
nix shell github:divnix/std
#+end_src
- Lets install git
#+begin_src tmux :session s1
nix-env -i git
#+end_src
- Lets clone our repo
#+begin_src tmux :session s1
git clone https://github.com/bernokl/nix-ops.git
#+end_src
- Lets cd into our directory and see what std gives us
#+begin_src tmux :session s1
cd nix-ops && std list
#+end_src
- Nice! I see the entire repo, going to run my server as confirmation.
#+begin_src tmux :session s1
std //cache-server/entrypoints/nix-cache-server:run
#+end_src
- I see the server but can not telnet to it.
- Lets disable iptables for a second
- Add this to /etc/nixos/configuration.nix
#+begin_example
networking.firewall.enable = false;
#+end_example
- Rebuild
#+begin_src tmux :session s1
nixos-rebuild switch
#+end_src
- Yas I can now start and telnet to port 8080 from remote machne
- Lets install docker
#+begin_src tmux :session s1
nix-env -i docker
#+end_src
- Lets enable daemon adding the followind to /etc/nixos/configuration.nix
#+begin_example
virtualisation.docker.enable = true;
users.users.<your-username>.extraGroups = [ "docker" ];
#+end_example
- lets spin up basic httpd container to check it works
#+begin_src tmux :session s1
docker run -d -p 8080:80 --name my-httpd-container httpd
#+end_src
- The above spins up a container we can test
- I can hit from curl and on remote ip
#+begin_src tmux :session s1
curl http://localhost:8080
#+end_src
- Lets stop the container and try our nix-ops container
#+begin_src tmux :session s1
docker stop my-httpd-container
#+end_src
- Lets try our container, first lets load the container in the local regestry
#+begin_src tmux :session s1
std //cache-server/containers/nix-cache-server:load
#+end_src
- Lets try and run it:
#+begin_src tmux :session s1
docker run -d nix-cache-server:yyvmmf9qzmjpl11sg0aly3svzyrjnjr0 -p 8080:8080
#+end_src
- It spins up the container, but no sign of bound ports
#+begin_example
 docker ps
CONTAINER ID   IMAGE                                               COMMAND                  CREATED         STATUS         PORTS     NAMES
50bda09a8ebb   nix-cache-server:yyvmmf9qzmjpl11sg0aly3svzyrjnjr0   "/bin/entrypoint -p â€¦"   6 seconds ago   Up 5 seconds             peaceful_germain
#+end_example
- Netstat -nat shows nothing bound on port 8080
- ps shows the container running the server:
#+begin_example
 ps faux | grep starm
root       17204  0.0  0.0   6624  2664 pts/0    S+   14:30   0:00  |           \_ grep starm
nobody     17177  0.2  0.2  19668 17184 ?        S    14:27   0:00      \_ starman master /nix/store/xgd2097cza1igzwq85rqf2dpak9086bg-nix-serve-20230307152850/libexec/nix-serve/nix-serve.psgi --port 8080
#+end_example
- So what the hell? why no bound ports on host?
- Ugg because publish/port needs to be defined before container, this works:
#+begin_src tmux :session s1
docker run -d -p 8080:8080 nix-cache-server:yyvmmf9qzmjpl11sg0aly3svzyrjnjr0 
#+end_src
- Locally
#+begin_src tmux :session s1
docker run -d -p 8080:8080 nix-cache-server:mdig60llqj2d6j2n8gj8yfbg1mjw3v7b
#+end_src
- Confirmation:
#+begin_example
> docker port 456dca6b623c

8080/tcp -> 0.0.0.0:8080
8080/tcp -> :::8080
#+end_example
- The above still does not work on osx. I do not know what makes the container crash, tried to run server-debug but that fails with an attempt to find the blob on docker.io.....
- Ummm, not sure how much time I want to spend troubleshooting osx issues
- Final Conclusion:
#+begin_example
- We now have repo that will std/flake build our server. Note we did not add docker build to devshell, std is enough for now.
- The same repo also now has capability to create oci image that we can run the server on a remote host.
#+end_example
- Next steps:
- I think I will use this same repo to add the entrypoint to microvm. I am still not sure about deploy of this arfifact, shoving a derivation into a container to run seems redundant, would like to understand real world use, do we need complex scheduling? Are we anticipating multiple services running independently, then we probably want a service mesh, but for purpose of caching and even production cardano nodes the deploy question seems secondary.
- If I have the microvm how do I want to depoy the caching service for use in yumi? What do I want to do about remote/local story sync? We talked about segmentation what does that mean? different directories for environments that can b nix copied?
** Spin caching server up in microvm
- Going to take learnings from container to spin up micorvm with same operable exposed on 8080
- Starting with example from https://github.com/divnix/std/blob/main/cells/lib/ops/mkMicrovm.md
#+begin_example
{
  inputs,
  cell,
}: let

   inherit (inputs) nixpkgs std;
   l = nixpkgs.lib // builtins;
   inherit (inputs.std.lib) ops;
   
in {

    myhost = ops.mkMicrovm ({ pkgs, lib, ... }: { networking.hostName = "microvms-host";});

}
#+end_example
- Update the flake with:
#+begin_example
 inputs.std.inputs.microvm.url =  "github:astro/microvm.nix";
#outputs:
 (std.blockTypes.microvms "microvms")

 microvms = std.harvest inputs.self [ "cache-server" "microvms" ];
 
#+end_example
- Got good error message by running "std re-cache" followed by "std list"
- It tells me I need to add microvm.nix as input in flake.nix like this:
#+begin_example
  inputs.std.inputs.microvm.url =  "github:astro/microvm.nix";
#+end_example
- And that seems to have satisfied it, a new std re-cache and std list now shows options for microvm/console/run trying console first:
- So far so good, building very hard on this vm, 10 minutes so far, looks like it is pulling around 2 gig of data so not ideal, but lets be patient and see what it gets
- And that seems to have satisfied it, a new std re-cache and std list now shows options for microvm/console/run trying console first:
- So far so good, building very hard on this vm, 10 minutes so far, looks like it is p7ulling around 2 gig of data so not ideal, but lets be patient and see what it gets
- My local machine ran out of space (had 12 gig free) so I spun it up on aws, here is the result:
#+begin_example
> std //cache-server/microvms/myhost:console

trace: warning: system.stateVersion is not set, defaulting to 23.05. Read why this matters on https://nixos.org/manual/nixos/stable/options.html#opt-system.stateVersion.
------------------------------------------------
Executing /root/.config/.local/state/last-action
With args []
------------------------------------------------
2023/04/21 21:49:47 socat[614299] E UNIX-CLIENT:microvms-host.sock: No such file or directory
No valid pty opened by qemu
#+end_example
- It does not look like it can find vittualization it needs, no qemu library, in a second :run attempt it could not find kvm, so it looks like the module has some unmet dependencies, I am not sure why, but considering the aws build on 4 cores and 8 gig of ram also took over an hour, I am not sure I want to pursue this much further.
- Status:
- I have options to console/run/microvm but none of them work.
#+begin_example
# It might be that this just needs more arguments
std //cache-server/microvms/myhost:microvm
trace: warning: system.stateVersion is not set, defaulting to 23.05. Read why this matters on https://nixos.org/manual/nixos/stable/options.html#opt-system.stateVersion.
------------------------------------------------
Executing /root/.config/.local/state/last-action
With args []
------------------------------------------------
/root/.config/.local/state/last-action: line 2: /nix/store/kpx759w1i79hnqi9bjlk27lw7illz3p1-microvm-qemu-microvms-host/bin/microvm-: No such file or directory

## let me go look what I see in that store path:
total 24
dr-xr-xr-x 2 root root 4096 Jan  1  1970 .
dr-xr-xr-x 4 root root 4096 Jan  1  1970 ..
lrwxrwxrwx 1 root root   79 Jan  1  1970 microvm-balloon -> /nix/store/b4ywqcm6hdsgs3xd1fczf69m5vqbj04y-microvm-balloon/bin/microvm-balloon
lrwxrwxrwx 1 root root   79 Jan  1  1970 microvm-console -> /nix/store/3nyz4gksnmmxdyvqp3rzgqw80p7hn37z-microvm-console/bin/microvm-console
lrwxrwxrwx 1 root root   71 Jan  1  1970 microvm-run -> /nix/store/0iyf0fq37nm71wn2jhfxajlk9gl74m0b-microvm-run/bin/microvm-run
lrwxrwxrwx 1 root root   81 Jan  1  1970 microvm-shutdown -> /nix/store/22zwv264l9g79w5sln38rbqnn4jzwa2h-microvm-shutdown/bin/microvm-shutdown

## Let me manually look at these:
[root@ip-172-31-0-29:~/nix-ops]# /nix/store/kpx759w1i79hnqi9bjlk27lw7illz3p1-microvm-qemu-microvms-host/bin/microvm-console
2023/04/24 12:58:24 socat[616629] E UNIX-CLIENT:microvms-host.sock: Connection refused
No valid pty opened by qemu

[root@ip-172-31-0-29:~/nix-ops]# /nix/store/kpx759w1i79hnqi9bjlk27lw7illz3p1-microvm-qemu-microvms-host/bin/microvm-run
char device redirected to /dev/pts/1 (label con1)
Could not access KVM kernel module: No such file or directory
qemu-system-x86_64: failed to initialize kvm: No such file or directory
#+end_example
- I am missing something on the micorvm, perhaps there is a build.
- At this point I want to move back to simply deploying the flake of the cache server using devshell. When we have use case for microvms that justifies the cost to spin up I will re-visit this build.
** Deploy caching-server instance on aws instance using devshell
- Goal is to create workflow for ops to interact with caching server in a meaningful way.
- 1st step will be to add commands for spinning up new instance in a devshel
- Lets declare env vars for our aws/secret/id
- TODO: Grab all our secrets from central store, one pass?
- I started by declaring this in .envar for now
#+begin_example
export AWS_SECRET_KEY=$(cat ~/.aws/credentials | grep secre | cut -d' ' -f3)
export AWS_ID=$(cat ~/.aws/credentials | grep "_id" | cut -d' ' -f3)
#+end_example
- I tried to wrap the above in -f to check for file, but it always returend blank even with file, envrc does not fail if file does not exists, so left it like this for now.
- ok, lets put very raw command in devshel, see what happens:
#+begin_example
ec2-run-instances -t t2.small --region ap-southeast-2 -W $AWS_SECRET_KEY  -O $AWS_ID -b '/dev/xvda=:25' -k gsg-keypair ami-0638db75ba113c635
#+end_example
- The final devshel has block added:
#+begin_example
      {
        name = "launch_aws";
        command = "ec2-run-instances -t t2.small --region ap-southeast-2 -W $AWS_SECRET_KEY  -O $AWS_ID -b '/dev/xvda=:25' -k gsg-keypair ami-0638db75ba113c635";
        help = "start a new aws instance using ec2-run";
        category = "Infra";
      }
#+end_example
- I can now run this with simple launch_aws in my shell.
- This is very nice, but now I need to consider how I manage machines spun up this way?
- It still feels more mature to let the above be tf command that we then use for managing the instance.

** explore nixops in more detail.
- Clone the repo
#+begin_src tmux :session s1
  nix-env -i git
  nix-env -i vim
  git clone https://github.com/NixOS/nixops.git
  # This needs to be looked at as well
  # git clone https://github.com/NixOS/nixops.git
#+end_src
- Enter directory, install nixops in that shell
#+begin_src tmux :session s1
cd nixops
nix-shell -p nixops
#+end_src
- It depends on python 2.7 that is no longer maintained so will need to allow insecure for now
- TODO: Figure out risk mittigation so we can install nixops without INSECURE:
#+begin_src tmux :session s1
export NIXPKGS_ALLOW_INSECURE=1
#+end_src
- Lets try again. 
#+begin_src tmux :session s1
nix-shell -p nixops
#+end_src
- This a big boy it has been installing for 10 minutes, not pegging any resources I can see just taking time.
- Going to read the intro and see if I can come up with game plan to test nixops for caching server deploy
- https://hydra.nixos.org/build/115931128/download/1/manual/manual.html
- This is what I learned so far :)
#+begin_example
nixops list
/nix/store/j7x9mwjmmnim4xhc7mvin9nprlv12gg8-python2.7-apache-libcloud-2.8.3/lib/python2.7/site-packages/libcloud/common/google.py:93: CryptographyDeprecationWarning: Python 2 is no longer supported by the Python core team. Support for it is now deprecated in cryptography, and will be removed in the next release.
  from cryptography.hazmat.backends import default_backend
+------+------+-------------+------------+------+
| UUID | Name | Description | # Machines | Type |
+------+------+-------------+------------+------+
+------+------+-------------+------------+------+
#+end_example
- commands available in help:
#+begin_example
positional arguments:
  operation            sub-command help
    list               list all known deployments
    create             create a new deployment
    modify             modify an existing deployment
    clone              clone an existing deployment
    delete             delete a deployment
    info               show the state of the deployment
    check              check the state of the machines in the network (note that this might alter the internal nixops state to consolidate with the real state of the resource)
    set-args           persistently set arguments to the deployment specification
    deploy             deploy the network configuration
    send-keys          send encryption keys
    destroy            destroy all resources in the specified deployment
    delete-resources   deletes the resource from the local NixOps state file.
    stop               stop all virtual machines in the network
    start              start all virtual machines in the network
    reboot             reboot all virtual machines in the network
    show-arguments     print the arguments to the network expressions
    show-physical      print the physical network expression
    ssh                login on the specified machine via SSH
    ssh-for-each       execute a command on each machine via SSH
    scp                copy files to or from the specified machine via scp
    mount              mount a directory from the specified machine into the local filesystem
    rename             rename machine in network
    backup             make snapshots of persistent disks in network (currently EC2-only)
    backup-status      get status of backups
    remove-backup      remove a given backup
    clean-backups      remove old backups
    restore            restore machines based on snapshots of persistent disks in network (currently EC2-only)
    show-option        print the value of a configuration option
    list-generations   list previous configurations to which you can roll back
    rollback           roll back to a previous configuration
    delete-generation  remove a previous configuration
    show-console-output
                       print the machine's console output on stdout
    dump-nix-paths     dump Nix paths referenced in deployments
    export             export the state of a deployment
    import             import deployments into the state file
    edit               open the deployment specification in $EDITOR
    copy-closure       copy closure to a target machine
    list-plugins       list the available nixops plugins
    unlock             Force unlock the deployment lock

#+end_example
- It looks like a mature project, can not find the date of the documents, but it looks valid. will compare it to readthedocs
- https://nixops.readthedocs.io/en/latest/introduction.html
- Boo, I need getting started I want basic deploy.nix for deploying example to aws
- This looks promising https://ops.functionalalgebra.com/nixops-by-example/ will follow for a bit,
- I tried to use the example deployment they had, but no joy, tried a few of the commands tweaking deploy
- I keep getting "could not find specified deployment in state file.
- I am also curious about nixops-aws, https://github.com/NixOS/nixops-aws
- Local install install takes a bit of time, not too crazy resource intensive, ends with error:
#+begin_example
  File "<frozen importlib._bootstrap>", line 1004, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'setuptools'
#+end_example
- Get the same error trying it on aws instance so issue with library or not supported for linux, lets try osx
- Nope same error with osx, something not working with the root branch of that repo, lets go see what has been most active
- Going back to nixops, following a few threads on generating nixops files
- Lets see if my new deploy works
#+begin_src tmux :session s1
nixops create -d my-unique-deployment-name -s /tmp/nixops/deploy.nixops
#+end_src
- No joy.
- At this point I am strugling to find good documentation to get a POC of aws deploy using nixops is supposed to work.
- This is very frustrating. I re-visit https://github.com/input-output-hk/cardano-node 
- Importing and using that flake feels like not that much of a lift, but I am still unsure what I would use to wrap the deploy, 
- It feels like I am missing something terraform will do lots of the lift on the aws side 
- Ending TF deploy with init-script that flake run/install of input-output-hk/cardano-node works
- I still wish my pipeline could be simpler, I wish I could talk to cardano-ops find out what their CI/CD pipeline looks like.
** Explore nixops through cardano repos
- Relevant links
- cardano-ops https://github.com/input-output-hk/cardano-ops - seems pretty active till end of Jan then nothing...
- ops-lib https://github.com/input-output-hk/ops-lib - had a commit two weeks ago
- nixops-aws https://github.com/input-output-hk/nixops-aws - old? 4y
- nixops https://github.com/input-output-hk/nixops - old? 10y
- An example of deployment even if it is from 2019 ops-lib: https://github.com/input-output-hk/ops-lib/blob/master/example/deployments/example-aws.nix
- For cardano-ops might be worth following up on the testnet example, https://github.com/input-output-hk/cardano-ops/blob/master/examples/shelley-testnet/README.md
- ci-world: https://github.com/input-output-hk/ci-world - current, looks like pipeline building, but I do not understand application.
- Work on iohk sre to learn process? Available positions:
#+begin_example
https://apply.workable.com/io-global/j/EF133E8D35/ sre - plutus - team member
https://apply.workable.com/io-global/j/BF1192A4DE/ sre - atala - team member
https://apply.workable.com/io-global/j/F8693FC8C7/ sre - lace - lead
#+end_example
- Engineering handbook
#+begin_src tmux :session s1
git clone https://github.com/input-output-hk/cardano-engineering-handbook.git
#+end_src
- cd in and start the shell
#+begin_src tmux :session s1
cd cardano-engineering-handbook
#+end_src
- O this is a policy handbook https://input-output-hk.github.io/cardano-engineering-handbook/introduction.html
- Good but not much use for our quest to nix deployment.
- Either go back to shelly-testnet example or return to our original goal.
- Although tempting the shelly testnet example does not get us closer to our goal of our own nix-ops repo
- STATUS: I really think everything we need is in these repos I just need a guide to see how I can use them. Leaving it off here for now.

** Manually deploy nix-ops, caching-server/cardano-node combining std with terraform 
- PLAN: provision AWS EC2-NixOS-AMI we then customize for our use with configuration.nix update
- First step would be to look into https://nixos.org/guides/deploying-nixos-using-terraform.html
- Interesting find from the above append this to main.tf simple way for configuration.nix control
- TODO: I would need to fork the tweag/terraform-nixos repo
#+begin_example
module "deploy_nixos" {
    source = "git::https://github.com/tweag/terraform-nixos.git//deploy_nixos?ref=5f5a0408b299874d6a29d1271e9bffeee4c9ca71"
    nixos_config = "${path.module}/configuration.nix"
    target_host = aws_instance.machine.public_ip
    ssh_private_key_file = local_file.machine_ssh_key.filename
    ssh_agent = false
#+end_example
- OK, lets create main.tf from tutorial and diypool inside terraform directory in nix-ops
- TODO: Need to create seperate terraform repo with structure to match environments, set up terragrunt, auth, key-management, s3 state file. Keep it simpel.
#+begin_src tmux :session s1
mkdir terraform
#+end_src
- Adding directory by function for now.
- TODO: Look at directory structure, compare to known good local structures you have, seperate env/modules etc.
#+begin_src tmux :session s1
mkdir caching-server
#+end_src
- Create main.tf from the deploying-nixos-using-terraform.html we are following
- Updating the region, release and port 22 cidr to be private ip we control
- Note in the file checked into the repo I removed the backend "remote" block, I want to set this up with s3.
- Also note this will create a private key on each build. We should change this to keys we care about.
- TODO: Set up TF s3 backend, key management?
- OK the main.tf and configuration.nix in terraform/caching-server/ will init/apply the machine with pkgs installed.
- Next I want to set those commands up through devshell, explore adding caching server flake to that configuration.nix
- Lets add ops commands for bringing up a machine, update our devshells.
#+begin_example
      { 
        name = "aws_terraform_init";
        # Need to make this command relative to the directory, not sure how we guarentee repo_root unless we use pwd to set env-var we can work from
        # All of this feels cludgy, I do not like adding tf into the mix like this, will keep exploring nixops
        #command = "terraform -chdir="./caching-server/" plan";
        command = "terraform init";
        help = "start a new aws instance using terraform";
        category = "Infra";
      }
#+end_example
- OK terraform init/plan/apply all works as expected.
- The new host comes up with vim and git already installed like we expressed in our configuration.nix
- Next step will be to see if we can get the caching-server flake to run using configuration.nix
- Oof this is a deep rabbit hole.
- I confirmed my server comes up as expected if I run:
#+begin_src tmux :session s1
nix run github:bernokl/nix-ops --extra-experimental-features nix-command --extra-experimental-features flakes
#+end_src
- But I can not find a simple way to run that command with configuration.nix
- I need to learn more about: https://github.com/nix-community/terraform-nixos/tree/master/deploy_nixos#readme
- I want to understand what I can do with it
- Also I know iohk repos are full of https://github.com/serokell/deploy-rs references.
- I want to compare the two
- Here is a quick summary:
#+begin_example
Prerequisites: Nix, Rust, Cargo
Create a new project:
"cargo new deploy-rs"
Add the deploy-rs dependency:
"cargo add serokell/deploy-rs"
Write a deploy.toml file:
This file specifies the targets to deploy to.
Write a main.rs file:
This file contains the code that will be deployed to the targets.
Build and deploy the project:
"cargo build"
"cargo deploy"

The the README also covers some additional topics, such as:
Using environment variables:
You can use environment variables to set the values of the target's attributes.
Deploying to multiple targets:
You can deploy to multiple targets by specifying a list of targets in the targets section of the deploy.toml file.
Deploying to a specific target:
You can deploy to a specific target by specifying the target's name in the targets section of the deploy.toml file.
#+end_example
- I need to understand what cargo does better.
- Current: compar deploy_nixos with deploy-rs 
** Test deploy-rs
- Steps provided by LLM
#+begin_example
Prerequisites: Nix, Rust, Cargo
Create a new project:
- cargo new deploy-rs
- Add the deploy-rs dependency:
   - cargo add serokell/deploy-rs
- Write a deploy.toml file:
   - This file specifies the targets to deploy to.
- Write a main.rs file:
   - This file contains the code that will be deployed to the targets.
- Build and deploy the project:
   - cargo build
   - cargo deploy
- The tutorial also covers some additional topics, such as:

- Using environment variables:
   - You can use environment variables to set the values of the target's attributes.
- Deploying to multiple targets:
   - You can deploy to multiple targets by specifying a list of targets in the targets section of the deploy.toml file.
- Deploying to a specific target:
   - You can deploy to a specific target by specifying the target's name in the targets section of the deploy.toml file.
#+end_example
- Talking to its competator I got instrucations to simply add these lines to my flake.nix
#+begin_example
deploy-rs.url = "github:serokell/deploy-rs";

deploy = {
          nodes = {
            myAWSInstance = {
              hostname = "example.com"; # Replace with your AWS instance's hostname or IP address
              user = "my-aws-username"; # Replace with your AWS username

              profiles = {
                hello = {
                  path = deploy-rs.lib.${system}.activate.custom self.defaultPackage.${system} "./bin/hello";
                };
              };
            };
          };
        };
#+end_example
- I wonder if I set that path to my server path from devshells if this will work? worth a shot
- I spent a couple of hours trying to integrate the various suggestions, but ended up turning to terraform for a bit.
- Status:
- I could not find enough documentation to figure out deploy-rs. Robert did however point me to https://github.com/input-output-hk/ci-ops which looks promising.
- I will re visit deploy-rs to see if I can get a better integration.
** Deploy caching ec2 and flake using terraform user_data
   - Lets get into our terraform directory
#+begin_src tmux :session s1 
cd terraform/caching-server
#+end_src
- To nix run our flake we will need to add some extraOptions.
- Add the following to terraform configuration.nix
#+begin_example
  nix.extraOptions = ''
      extra-experimental-features = nix-command
      extra-experimental-features = flakes
    '';
#+end_example
- Now to test our user_data block, add the following to the main.tf/aws_instance block
- Note the content of the EOL file has no space in front of it
- Also important to note the env bash -xe, because /bin/bash failed, but it allowed env I changed it
#+begin_example
    user_data = <<-EOL
#!env bash -xe
# Orig with options being passed in before the configuration.nix update
#nix run github:bernokl/nix-ops --no-write-lock-file --extra-experimental-features nix-command --extra-experimental-features flakes &>/tmp/outNix
nix run github:bernokl/nix-ops --no-write-lock-file &> /tmp/nixOutput 
EOL
}
#+end_example
- If this works the goal will be to put the content of the script into a seperate file
- Lets go deploy our terraform and see what we got
- NOTE: Everytime you apply this it will restart the machine meaning currently you will get in ip
#+begin_src tmux :session s1
aws_terraform_apply
#+end_src
- MM we got an error:
#+begin_example
Error: local-exec provisioner error
#+end_example
- Eneded up having to move --no-write-log to user_data, above is correct
- Lets go see what we got, grab the ip from the aws-console
#+begin_src tmux :session s1
ssh -i id_rsa.pem root@xx.xx.xx.xx 
#+end_src
- Lets go see if we have a working store
#+begin_src tmux :session s1
nix store ping --store http://127.0.0.1:8080 
#+end_src
- YAS!!
#+begin_example
Store URL: http://127.0.0.1:8080
#+end_example
- Lets whitelist my ip in iptables and see if our security group will let us hit the server from home
#+begin_src tmux :session s1
iptables -I INPUT 1 -s xx.xx.xx.xx/32 -j ACCEPT
#+end_src
- Logged out of the machine and try:
#+begin_src tmux :session s1
nix store ping --store http://xx.xx.xx.xx:8080 
#+end_src
- YAS! our store is up and can be hit remotely.
- Simple way to find out what it is serving:
#+begin_src tmux :session s1
  lsof /nix/store | grep starm
  ps faux | grep starma
#+end_src
- Compare the pids between the two, you 
- Initailly I could not figure out why I could not connect, this is simple test that port is being reached
#+begin_src tmux :session s1
tcpdump -i any port 8080
#+end_src

* Status:
** In the current state the terraform apply will:
- Deploy ec2 instance
- Apply local configuration.nix to remote machine
- Run user_data which currently installs and runs the nix-cache server
- At the end of the process I have a working cache server in aws I can use in local or other aws instances as they come up.
- We are not deploying this right now because running a big enough instance 24-7 will not add enough value to our current dev cycle (there is already a ssh-sync inside yumi).
** TODO:
- Add efs to make nix-store persistent, or do we just want to know with s3 backup solution? 
- S3 backups is documented and might be simpler at this point, I would need to see how we use store to decide. 
- Add tailscale and get machine to register with yumi tailscale network
- Start values file for vars like machine type, whitelisting etc
- Fix key management, right now new key gets generated for the new machine created. Key rotation? 1Pass? other?
- If we want to keep std menu commands we need to fix pathing (ie terraform can only run from tf dir. How do we know where the menu is being called from? I am torn, I like that the devshell is easy way to see what we do with tools, but it also feels like a layer of complexity that does not expose everything you can do. 
- Create rsync between remote efs/S3? and dev stores on NAS
- We should have loadbalancer defined for this service, if s3/efs holds copy of store do we care about uptime?
- We should have gateway we route all the traffic out through so we can do whitelisting for a consistent ip for services like tailscale.
** For further consideration:
- I think the nix-serve-flake we run can be vastly simplified, what we call https://github.com/edolstra/nix-serve, it has some options for signing packages that we might care about as caches prolifirates, I am not sure if there is additional value over just nixpkgs.nix-serve I do not understand, but it is simple to use.
- When I started this project I did not understand how simple it is to just nixpkgs.nix-serve any store, there is 4 very simple alternatives to what we have. At some point it would be good to evaluate this in context of data syncing https://nixos.org/manual/nix/stable/package-management/binary-cache-substituter.html
- With understanding I have not I would like to re-visit this entire architecture, there are lots more possibilities I now see
   
